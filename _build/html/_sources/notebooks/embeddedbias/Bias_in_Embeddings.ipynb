{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded bias\n",
    "\n",
    "  - How to deal with words in mathematical models?\n",
    "\n",
    "  > represent words with numbers!\n",
    "\n",
    "\n",
    "## Method 1: bag-of-words\n",
    "\n",
    "  - make list of occurring words and give them numbers ordered by frequency\n",
    "    - 'the' -> `1`\n",
    "    - 'it' -> `2`\n",
    "    - ...\n",
    "    - 'embedding' -> `23579`\n",
    "\n",
    "  - Problems:\n",
    "    - huge _one-hot-vectors_\n",
    "    - hard to train\n",
    "    - no context embedded\n",
    "\n",
    "## Method 2: Unsupervised word-vectors\n",
    "\n",
    "  - use word and its context in reduced vector space (e.g. 300 dim.)\n",
    "    - train model such that similar context leads to similar vectors\n",
    "\n",
    "  - Pro: Semantics partly encoded in numbers\n",
    "  - Problems:\n",
    "    - large and very clean corpora necessary\n",
    "    - computational hard\n",
    "\n",
    "\n",
    "## General problems\n",
    "\n",
    "  - No result for out-of-corpus words\n",
    "  - Bad for specific language of e.g. scientific terms\n",
    "  - Word embeddings are based on contemporary texts\n",
    "  - If specific words occur in a limited set of contexts, bias translates from text to numbers\n",
    "    - 'man' occurs with 'doctor', but 'woman' occurs with 'nurse'\n",
    "    - 'black' occurs with 'criminal'\n",
    "    - 'muslim' with 'radical'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the embedded biasword\n",
    "\n",
    "Based on the blog post:\n",
    " \n",
    "  - Marco Peixeiro (2019) [Introduction to NLP and bias in AI](https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-and-bias-in-ai-877d3f3ee680), TowardsDataScience-Blog\n",
    "  - T. Manzini et.al (2019) [Black is to Criminal as Caucasian is to Police](https://www.aclweb.org/anthology/N19-1062/), ACL Anthology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe dataset contains lines with words and their vectors. You have to download the dataset from the source page: [Source](https://nlp.stanford.edu/projects/glove/) \n",
    "\n",
    "Choose the smallest dataset, since this is already around 800MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word2vectors = read_glove_vecs('../data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to compare vectors is to calculate their cosine similarity ([wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity)):\n",
    "\n",
    "  - similar vectors point in similar directions, the angle between them is small\n",
    "  - equal vectors have cosine equals one\n",
    "  - exactly opposite vectors have cosine equals minus one\n",
    "  - unrelated vectors can have cosine equal zero\n",
    "  \n",
    "Formular:\n",
    "\n",
    " > $\\cos(\\theta) = \\frac{\\textbf{U} \\cdot \\textbf{V}}{||\\textbf{U}|| \\cdot || \\textbf{V}|| }$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using _numpy_ package we can encode this as a function. \n",
    "\n",
    "The parameters _disp_ and _multi_ allow to change the output or input format. If _disp=True_ the function will print a string with the input words and the calculated value, if its off (default) only the calculated similarity is returned. If _multi=True_ the input is expected as two lists, with two words each. The word vectors are then substracted from each other and the similarity of the new values is calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2, disp=False, multi=False):\n",
    "    if multi:\n",
    "        u = word2vectors[word1[1].lower()] - word2vectors[word1[0].lower()]\n",
    "        v = word2vectors[word2[1].lower()] - word2vectors[word2[0].lower()]\n",
    "    else:\n",
    "        u = word2vectors[word1.lower()]\n",
    "        v = word2vectors[word2.lower()]\n",
    "    cossim = np.dot(u, v) / (np.sqrt(np.sum(u**2)) * np.sqrt(np.sum(v**2)))\n",
    "    if disp:\n",
    "        return print('cos_sim({0}, {1}) = {2}'.format(word1, word2, cossim))\n",
    "    return cossim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with word vectors\n",
    "\n",
    "  - Experiment with the function!\n",
    "  - How does the similarity of (father,mother) compares to e.g. (rocket,cake)?\n",
    "  - Calculate the similarity for ['rome','italy'] and ['france','paris'].\n",
    "  - Why does the sign change for ['rome','italy'],['paris','france'] ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(???,???,disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(???,???,disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([???,???], [???,???], disp=True, multi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there semantics captured ?\n",
    "\n",
    "To have a look at semantics, we have to be able to analyse relations like\n",
    "\n",
    "  > Man is to woman as king is to X (e.g. queen)\n",
    "  \n",
    "Since words are now encoded as vectors, we can calculate `(woman - man)`, which forms a new vector. Since we are looking for a word `X` such that `(X - king)` is similar to `(woman - man)`, we can iteratively scan all words of the corpus. \n",
    "\n",
    "If the cosine similarity of `cos_sim((woman-man),(X-king))` for a target word `X` is larger then the similarity of the word before, we take not of the word and the similarity and repeat. \n",
    "\n",
    "In the end, the word with the largest similarity is returned. If _disp=True_ the input words are printed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word1, word2, word3, disp=True):\n",
    "    word1, word2, word3 = word1.lower(), word2.lower(), word3.lower()\n",
    "    words = word2vectors.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "    for w in words:           \n",
    "        if w in [word1, word2, word3] :\n",
    "            continue\n",
    "        cosine_sim = cosine_similarity([word1,word2], [word3,w], multi=True)\n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "    if disp:\n",
    "        return print ('{0} -> {1} :: {2} -> {3}'.format(word1,word2,word3, best_word ))\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics:\n",
    "  \n",
    "  - Test if embeddings can capture semantics\n",
    "  - What is the best word for the trio (italy, italian, spain) ?\n",
    "  - And for (germany,berlin,austria) ?\n",
    "  \n",
    "What about bias? \n",
    "\n",
    "  - What is the best word for (woman, nurse, man)? \n",
    "  - And for (christian, civilians, muslim)? \n",
    "  - Discuss these findings\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_analogy(???,???,???, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a _gender_ vector? \n",
    "\n",
    "Take for example the difference between woman and man. Does this encode a \"gender\" vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecCosSimilarity(u,v):\n",
    "    cossim = np.dot(u, v) / (np.sqrt(np.sum(u**2)) * np.sqrt(np.sum(v**2)))\n",
    "    return cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = word2vectors['woman'] - word2vectors['man']\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Check for a range of names if there is a common trend in male or female names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "name_list = ['john','eve','priya']\n",
    "for w in name_list:\n",
    "    print (w, vecCosSimilarity(word2vectors[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Do the same for charged words, e.g. fashion, guns, etc.\n",
    "  - Do you see a trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['actor','actress','doctor','nurse']\n",
    "for w in word_list:\n",
    "    print (w, vecCosSimilarity(word2vectors[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to change this ?\n",
    "\n",
    "Not shown, but a simple projection works, if we assume that gender is indeed encoded in `g`.\n",
    "\n",
    "\n",
    "\n",
    "![Basic idea, source _deeplearning.ai_](./assets/bias_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we use ML to detect bias ?\n",
    "\n",
    "  - Pre-process Tucholsky corpus with NLP tools\n",
    "  - Train GloVe on Tucholsky\n",
    "\n",
    "  - Exercise:\n",
    "    - Can we see a bias in Tucholsky's language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outlook: Intellectual dept of software\n",
    "\n",
    "We have stepped in the area of _unknown software_ and silently started using programs without understanding them\n",
    "\n",
    "This has lots of interesting implications and philosophical dimensions!\n",
    "\n",
    "- Cory Doctorow [Blog-Post](https://boingboing.net/2019/07/28/orphans-of-the-sky.html)\n",
    "  - Technology on top of other technology hides technological dept\n",
    "- Jonathan Zittrain [New Yorker Article](https://www.newyorker.com/tech/annals-of-technology/the-hidden-costs-of-automated-thinking)\n",
    "  - Main argument: AI is applied science and limits basic research\n",
    "- Tyler Vigen [Collection of Correlations](http://www.tylervigen.com/spurious-correlations)\n",
    "  - More data creates also more spurious correlations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
